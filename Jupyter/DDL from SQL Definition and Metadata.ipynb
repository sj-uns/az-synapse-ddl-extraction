{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sqlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory of Repo where the SQL DDL will be saved\n",
    "repo_dir = input(\"Enter Repo directory path, to save the repo files:\")\n",
    "table_metadata_filepath = input(\"Enter Table Metadata file path:\")\n",
    "object_definitions_filepath = input(\"Enter Object Definitions file path:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table DDL Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate_create_table_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the CREATE TABLE script\n",
    "def generate_create_table_script(df):\n",
    "    # Initialise the varchar_max_check:\n",
    "        # This will help with creating the DDL where a varchar(max) has been found in the SQL\n",
    "    varchar_max_check = False\n",
    "\n",
    "    # Get the Schema and Table name for the table being parsed\n",
    "    schema_name = df['SchemaName'].iloc[0]\n",
    "    table_name = df['TableName'].iloc[0]\n",
    "    \n",
    "    # Initialise the CREATE TABLE Script, with the Schema and Table name\n",
    "    create_table_script = f\"CREATE TABLE [{schema_name}].[{table_name}]\\n(\\n\"\n",
    "    \n",
    "    # Iterate through the rows in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        ## Columns and Data Types ##\n",
    "        column_definition = f\"\\t[{row['ColumnName']}] [{row['DataType']}]\"\n",
    "        \n",
    "        ### Strings ###\n",
    "        if row['DataType'] in ['varchar', 'nvarchar', 'varbinary', 'char']:\n",
    "\n",
    "            # Look for varchar(max) fields\n",
    "            if row['MaxLength'] == -1:\n",
    "                varchar_max_check = True\n",
    "                column_definition += \"(MAX)\"\n",
    "            else:\n",
    "                column_definition += f\"({row['MaxLength']})\"\n",
    "\n",
    "        ### Numbers / Decimal ###\n",
    "        elif row['DataType'] in ['decimal', 'numeric']:\n",
    "            column_definition += f\"({row['precision']},{row['scale']})\"\n",
    "\n",
    "        ### Dates (datetime2) ###\n",
    "        elif row['DataType'] == 'datetime2':\n",
    "            column_definition += f\"({row['scale']})\"\n",
    "        \n",
    "\n",
    "        ## Determine identity columns ##\n",
    "        if row['is_identity']:\n",
    "            column_definition += \" IDENTITY(1,1)\"\n",
    "        \n",
    "\n",
    "        ## Determine NULL / NOT NULL columns ##\n",
    "        if row['is_nullable']:\n",
    "            column_definition += \" NULL\"\n",
    "        else:\n",
    "            column_definition += \" NOT NULL\"\n",
    "        \n",
    "\n",
    "        ## For each column definition enter a new line ##\n",
    "        create_table_script += column_definition + \",\\n\"\n",
    "    \n",
    "\n",
    "    ## Strip any additional new line characters ##\n",
    "    create_table_script = create_table_script.rstrip(\",\\n\") + \"\\n)\\n\"\n",
    "    \n",
    "\n",
    "    ## Table Distribution Settings ##\n",
    "    distribution_type = df[\"DistributionType\"].iloc[0]\n",
    "    distribution_col = df[\"HashDistributionColumnName\"].iloc[0]\n",
    "\n",
    "    ### Obtain Hash Distribution Columns ###\n",
    "    if distribution_type == 'HASH':\n",
    "        if distribution_col:\n",
    "            distribution_type += f\"([{distribution_col}])\"\n",
    "        else:\n",
    "            raise ValueError(f\"No column provided for Hash distributed table: [{schema_name}].[{table_name}]!\")\n",
    "\n",
    "\n",
    "    ## Table Indexing Settings ##\n",
    "    index_type = df[\"IndexType\"].iloc[0]\n",
    "\n",
    "\n",
    "    ## Add Table Distribution and Index Settings to the CREATE TABLE Script ##\n",
    "    if varchar_max_check:\n",
    "        create_table_script += f\"WITH\\n(\\n\\tDISTRIBUTION = {distribution_type},\\n\\tHEAP\\n)\" # If varchar(max) detected then use HEAP Index\n",
    "    else:\n",
    "        if index_type:\n",
    "            create_table_script += f\"WITH\\n(\\n\\tDISTRIBUTION = {distribution_type},\\n\\t{index_type} INDEX\\n)\"\n",
    "        else:\n",
    "            create_table_script += f\"WITH\\n(\\n\\tDISTRIBUTION = {distribution_type},\\n\\tHEAP\\n)\" # If no index setting found then use HEAD Index as default\n",
    "    \n",
    "    return create_table_script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_table_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table_metadata(repo_dir, df_metadata):\n",
    "    # Split the dataframe by each table in the TABLE_NAME field\n",
    "    tables = df_metadata.groupby(['SchemaName', 'TableName'])\n",
    "\n",
    "    table_count = 0\n",
    "\n",
    "    # Iterate over each table and generate the CREATE TABLE script\n",
    "    for (schema_name, table_name), table_df in tables:\n",
    "        # Generate the CREATE TABLE script\n",
    "        create_table_script = generate_create_table_script(table_df)\n",
    "        \n",
    "        # Create the sub-folder named after the schema for the table\n",
    "        folder_path = os.path.join(repo_dir, schema_name, \"Tables\")\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        \n",
    "        # Save the generated CREATE TABLE SQL script as a .sql file\n",
    "        file_path = os.path.join(folder_path, f\"{table_name}.sql\")\n",
    "        with open(file_path, 'w') as file:\n",
    "            file.write(create_table_script)\n",
    "\n",
    "        table_count += 1\n",
    "\n",
    "    print(f\"\"\"\n",
    "        CREATE TABLE scripts have been generated and saved successfully.\n",
    "        {table_count} table DDL files processed\n",
    "        \"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object DDL Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parse_object_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_object_definitions(repo_dir, df_definitions):\n",
    "    # Group the dataframe by 'ObjectType'\n",
    "    obj_grouped = df_definitions.groupby('ObjectType')\n",
    "\n",
    "    # Run through each object type\n",
    "    for object_type, objects_df in obj_grouped:\n",
    "        object_count = 0\n",
    "\n",
    "        # Run through each object in the group\n",
    "        for _, row in objects_df.iterrows():\n",
    "            schema_name = row['SchemaName']\n",
    "            object_name = row['ObjectName']\n",
    "            object_definition = row['ObjectDefinition']\n",
    "            \n",
    "            formatted_view = sqlparse.format(object_definition, reindent=False, keyword_case='upper')\n",
    "            \n",
    "            # Create the sub-folder named after the schema for the view\n",
    "            folder_path = os.path.join(repo_dir, schema_name, object_type)\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "            \n",
    "            # Save the generated CREATE VIEW SQL script as a .sql file\n",
    "            file_path = os.path.join(folder_path, f\"{object_name}.sql\")\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.write(formatted_view)\n",
    "\n",
    "            object_count += 1\n",
    "\n",
    "        print(f\"\"\"\n",
    "            CREATE scripts for {str(object_type).upper()} have been generated and saved successfully.\n",
    "            {object_count} DDL files processed\n",
    "            \"\"\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import SQL Metadata / Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Table Metadata\n",
    "table_metadata = pd.read_json(fr\"{table_metadata_filepath}\")\n",
    "\n",
    "# Display Table Metadata\n",
    "table_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Object Definitions\n",
    "object_definitions = pd.read_json(fr\"{object_definitions_filepath}\")\n",
    "\n",
    "# Display Object Definitions\n",
    "object_definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Metadata / Definitions to Repo Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_table_metadata(fr\"{repo_dir}\", table_metadata)\n",
    "parse_object_definitions(fr\"{repo_dir}\", object_definitions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
